{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain RAG: From Basics to Production-Ready RAG Chatbot\n",
    "\n",
    "**Updated for LangChain v1.2.4 (January 2026)**\n",
    "\n",
    "This notebook is a modernized version of the [FutureSmart.ai RAG Tutorial](https://blog.futuresmart.ai/langchain-rag-from-basics-to-production-ready-rag-chatbot), updated to use the latest LangChain syntax and best practices.\n",
    "\n",
    "## What's New in This Version\n",
    "- âœ… Uses `create_agent` from `langchain.agents` (modern agent API)\n",
    "- âœ… Modern `create_retriever_tool` from `langchain_core.tools`\n",
    "- âœ… `InMemorySaver` checkpointer for conversation memory\n",
    "- âœ… GPT-4o vision support for PDFs with diagrams/tables\n",
    "- âœ… Includes local model alternatives (Ollama)\n",
    "- âœ… Advanced PDF processing (tables, diagrams, images)\n",
    "\n",
    "## Tutorial Outline\n",
    "1. Setup & Installation\n",
    "2. LangChain Basics (LLM, Prompts, LCEL)\n",
    "3. Document Processing (including vision-based PDF extraction)\n",
    "4. Vector Store & Retriever\n",
    "5. Building a RAG Agent (Modern Approach)\n",
    "6. Conversational RAG with Memory\n",
    "7. Production Considerations (Streaming, Multi-tool Agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Installation\n",
    "\n",
    "Install the required packages. We use the modular LangChain packages for better dependency management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (LangChain v1.2.4, January 2026)\n",
    "# NOTE: Run this cell once, then restart runtime if needed\n",
    "\n",
    "# Core packages\n",
    "!pip install -qU 'langchain>=1.2.4' langchain-openai langchain-chroma langgraph\n",
    "!pip install -qU langchain-community pypdf docx2txt sentence-transformers\n",
    "\n",
    "# For advanced PDF processing (tables, diagrams, images)\n",
    "!pip install -qU pymupdf pillow pdfplumber\n",
    "\n",
    "# For local models using Ollama (optional but recommended)\n",
    "!pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your API keys\n",
    "import os\n",
    "\n",
    "# Option 1: Set directly (not recommended for production)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n",
    "\n",
    "# Option 2: Load from .env file (recommended)\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. LangChain Basics\n",
    "\n",
    "Before diving into RAG, let's understand the core LangChain components.\n",
    "\n",
    "### 2.1 Working with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Using OpenAI's GPT-4o-mini (cost-effective choice for tutorials)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Simple invocation\n",
    "response = llm.invoke(\"What is Retrieval Augmented Generation?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVE: Using local models with Ollama\n",
    "# Ollama provides free, local inference with models like Llama 3.2, DeepSeek, Mistral, etc.\n",
    "\n",
    "# To use Ollama:\n",
    "# 1. Install Ollama: https://ollama.com/download\n",
    "# 2. Pull a model: ollama pull llama3.2\n",
    "# 3. Uncomment the code below\n",
    "\n",
    "USE_OLLAMA = False  # Set to True to use local Ollama instead of OpenAI\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    from langchain_ollama import ChatOllama\n",
    "    \n",
    "    # For text-only tasks, use a standard model\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.2\",      # or \"mistral\", \"deepseek-r1:8b\", \"qwen2.5\"\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # For vision/multimodal tasks (PDFs with images), use a vision model\n",
    "    llm_vision = ChatOllama(\n",
    "        model=\"llava\",         # or \"llava:13b\", \"bakllava\"\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    response = llm.invoke(\"What is RAG?\")\n",
    "    print(\"Ollama response:\", response.content)\n",
    "else:\n",
    "    print(\"Using OpenAI (set USE_OLLAMA=True to use local Ollama)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prompt Templates\n",
    "\n",
    "Prompt templates help structure our prompts with dynamic variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are an expert on {topic}. Explain {concept} in simple terms.\"\n",
    ")\n",
    "\n",
    "# Format the prompt\n",
    "formatted_prompt = prompt.format(topic=\"machine learning\", concept=\"embeddings\")\n",
    "print(\"Formatted prompt:\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Output Parsers\n",
    "\n",
    "Output parsers help structure the LLM's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# StrOutputParser extracts just the text content\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Parse the response\n",
    "parsed_output = output_parser.parse(response.content)\n",
    "print(type(parsed_output))  # <class 'str'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 LCEL (LangChain Expression Language) Chains\n",
    "\n",
    "LCEL allows us to chain components together using the pipe (`|`) operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an LCEL chain: prompt -> llm -> parser\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Invoke the chain\n",
    "result = chain.invoke({\"topic\": \"AI\", \"concept\": \"vector databases\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Structured Output (Pydantic)\n",
    "\n",
    "For more complex use cases, we can get structured JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define the output structure\n",
    "class ConceptExplanation(BaseModel):\n",
    "    concept: str = Field(description=\"The concept being explained\")\n",
    "    definition: str = Field(description=\"A concise definition\")\n",
    "    key_points: List[str] = Field(description=\"Key points to remember\")\n",
    "    difficulty: str = Field(description=\"Difficulty level: beginner, intermediate, advanced\")\n",
    "\n",
    "# Use structured output\n",
    "structured_llm = llm.with_structured_output(ConceptExplanation)\n",
    "\n",
    "# Get structured response\n",
    "structured_response = structured_llm.invoke(\n",
    "    \"Explain what a Vector Database is and why it's important for RAG\"\n",
    ")\n",
    "print(f\"Concept: {structured_response.concept}\")\n",
    "print(f\"Definition: {structured_response.definition}\")\n",
    "print(f\"Key Points: {structured_response.key_points}\")\n",
    "print(f\"Difficulty: {structured_response.difficulty}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Document Processing\n",
    "\n",
    "To build a RAG system, we need to load and process documents.\n",
    "\n",
    "### 3.1 Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "\n",
    "# Example: Load a PDF file\n",
    "# Replace with your own file path\n",
    "pdf_path = \"sample_document.pdf\"  # <-- Update this path\n",
    "\n",
    "# Uncomment to use with a real PDF:\n",
    "# loader = PyPDFLoader(pdf_path)\n",
    "# documents = loader.load()\n",
    "# print(f\"Loaded {len(documents)} pages\")\n",
    "\n",
    "# For this demo, we'll create sample documents\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Sample documents for demonstration\n",
    "sample_documents = [\n",
    "    Document(\n",
    "        page_content=\"LangChain is a framework for developing applications powered by language models. It provides tools for prompt management, chains, and agents.\",\n",
    "        metadata={\"source\": \"langchain_intro.txt\", \"page\": 1}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"RAG (Retrieval Augmented Generation) combines retrieval and generation to produce more accurate and up-to-date responses. It works by retrieving relevant documents from a knowledge base.\",\n",
    "        metadata={\"source\": \"rag_overview.txt\", \"page\": 1}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector databases store data as high-dimensional vectors, enabling similarity search. Popular options include Chroma, Pinecone, and Weaviate.\",\n",
    "        metadata={\"source\": \"vector_db.txt\", \"page\": 1}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Embeddings are numerical representations of text that capture semantic meaning. OpenAI embeddings and sentence-transformers are commonly used.\",\n",
    "        metadata={\"source\": \"embeddings.txt\", \"page\": 1}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangGraph is a library for building stateful, multi-actor applications. It powers LangChain's agent framework with features like persistence and streaming.\",\n",
    "        metadata={\"source\": \"langgraph.txt\", \"page\": 1}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Created {len(sample_documents)} sample documents for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Advanced PDF Loading: Tables, Diagrams, and Images\n",
    "\n",
    "Standard PDF loaders only extract text. For PDFs with **tables, diagrams, flowcharts, or images**, we need specialized approaches:\n",
    "\n",
    "1. **Unstructured** - Best for complex layouts with tables\n",
    "2. **PDFPlumber** - Excellent table extraction\n",
    "3. **Vision Models (GPT-4o)** - Best for diagrams and visual content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ADVANCED PDF PROCESSING: Tables, Diagrams, and Images\n",
    "# ============================================================\n",
    "# For production PDFs with complex layouts, install these:\n",
    "# !pip install -qU unstructured[pdf] pdf2image pdfplumber pymupdf pillow\n",
    "\n",
    "# Option 1: Unstructured - Best for complex layouts and tables\n",
    "# ------------------------------------------------------------\n",
    "\"\"\"\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "# hi_res mode uses vision models to understand layout\n",
    "loader = UnstructuredPDFLoader(\n",
    "    \"complex_document.pdf\",\n",
    "    mode=\"elements\",           # Preserves document structure\n",
    "    strategy=\"hi_res\",         # Uses OCR + layout detection\n",
    "    extract_images_in_pdf=True # Extracts embedded images\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Elements are categorized: Title, NarrativeText, Table, Image, etc.\n",
    "for doc in docs:\n",
    "    print(f\"Type: {doc.metadata.get('category')}\")\n",
    "    print(f\"Content: {doc.page_content[:100]}...\")\n",
    "\"\"\"\n",
    "\n",
    "# Option 2: PDFPlumber - Excellent for table extraction\n",
    "# ------------------------------------------------------------\n",
    "\"\"\"\n",
    "import pdfplumber\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def extract_tables_from_pdf(pdf_path):\n",
    "    tables_as_docs = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            tables = page.extract_tables()\n",
    "            for j, table in enumerate(tables):\n",
    "                # Convert table to markdown format\n",
    "                if table:\n",
    "                    headers = table[0]\n",
    "                    rows = table[1:]\n",
    "                    md_table = \"| \" + \" | \".join(str(h) for h in headers) + \" |\\\\n\"\n",
    "                    md_table += \"| \" + \" | \".join(\"---\" for _ in headers) + \" |\\\\n\"\n",
    "                    for row in rows:\n",
    "                        md_table += \"| \" + \" | \".join(str(cell) for cell in row) + \" |\\\\n\"\n",
    "                    \n",
    "                    tables_as_docs.append(Document(\n",
    "                        page_content=md_table,\n",
    "                        metadata={\"source\": pdf_path, \"page\": i+1, \"table\": j+1, \"type\": \"table\"}\n",
    "                    ))\n",
    "    return tables_as_docs\n",
    "\n",
    "# Usage:\n",
    "# table_docs = extract_tables_from_pdf(\"report_with_tables.pdf\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Advanced PDF loading methods defined (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Vision-Based PDF Processing with GPT-4o\n",
    "\n",
    "For PDFs with **diagrams, flowcharts, architecture diagrams, or infographics**, the best approach is to:\n",
    "1. Convert PDF pages to images\n",
    "2. Use **GPT-4o** (vision model) to understand and describe the visual content\n",
    "3. Store the descriptions for RAG retrieval\n",
    "\n",
    "**GPT-4o** is the recommended model because:\n",
    "- Native multimodal support (text + images)\n",
    "- Excellent at understanding diagrams, charts, and tables\n",
    "- Can extract structured information from visual content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GPT-4o VISION: Process PDFs with Diagrams and Visual Content\n",
    "# ============================================================\n",
    "# Required: pip install pdf2image pymupdf pillow\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Use GPT-4o for vision tasks (best for diagrams/tables)\n",
    "# GPT-4o has native multimodal support - handles text AND images\n",
    "vision_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",  # Full GPT-4o for best visual understanding\n",
    "    temperature=0,\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "def process_pdf_with_vision(pdf_path: str) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Process a PDF by converting pages to images and using GPT-4o\n",
    "    to extract text, describe diagrams, and understand visual content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import fitz  # PyMuPDF\n",
    "        from PIL import Image\n",
    "    except ImportError:\n",
    "        print(\"Install required packages: pip install pymupdf pillow\")\n",
    "        return []\n",
    "    \n",
    "    documents = []\n",
    "    pdf = fitz.open(pdf_path)\n",
    "    \n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]\n",
    "        \n",
    "        # Convert page to image (high resolution for better OCR)\n",
    "        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x zoom\n",
    "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "        \n",
    "        # Convert to base64 for GPT-4o\n",
    "        buffer = BytesIO()\n",
    "        img.save(buffer, format=\"PNG\")\n",
    "        img_base64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "        \n",
    "        # Use GPT-4o to analyze the page\n",
    "        message = HumanMessage(\n",
    "            content=[\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"\"\"Analyze this PDF page and extract ALL content:\n",
    "\n",
    "1. **Text Content**: Extract all readable text, preserving structure\n",
    "2. **Tables**: If there are tables, convert them to markdown format\n",
    "3. **Diagrams/Flowcharts**: Describe any diagrams, flowcharts, or visual elements in detail\n",
    "4. **Images**: Describe any images and their relevance to the content\n",
    "5. **Key Information**: Highlight important data, figures, or conclusions\n",
    "\n",
    "Format your response clearly with sections for each type of content found.\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/png;base64,{img_base64}\",\n",
    "                        \"detail\": \"high\"  # High detail for better accuracy\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        response = vision_llm.invoke([message])\n",
    "        \n",
    "        documents.append(Document(\n",
    "            page_content=response.content,\n",
    "            metadata={\n",
    "                \"source\": pdf_path,\n",
    "                \"page\": page_num + 1,\n",
    "                \"processing\": \"gpt-4o-vision\",\n",
    "                \"has_visual_content\": True\n",
    "            }\n",
    "        ))\n",
    "        \n",
    "        print(f\"Processed page {page_num + 1}/{len(pdf)}\")\n",
    "    \n",
    "    pdf.close()\n",
    "    return documents\n",
    "\n",
    "# Example usage (uncomment with a real PDF):\n",
    "# vision_docs = process_pdf_with_vision(\"diagram_heavy_document.pdf\")\n",
    "# print(f\"Processed {len(vision_docs)} pages with vision analysis\")\n",
    "\n",
    "print(\"GPT-4o vision processing function defined!\")\n",
    "print(\"Use: vision_docs = process_pdf_with_vision('your_pdf.pdf')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Splitting Documents\n",
    "\n",
    "Large documents need to be split into smaller chunks for effective retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configure the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Maximum characters per chunk\n",
    "    chunk_overlap=100,     # Overlap between chunks for context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Split priority\n",
    ")\n",
    "\n",
    "# Split documents (for demo, our docs are already small)\n",
    "splits = text_splitter.split_documents(sample_documents)\n",
    "print(f\"Split into {len(splits)} chunks\")\n",
    "\n",
    "# Preview a chunk\n",
    "print(f\"\\nSample chunk content:\\n{splits[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Vector Store & Retriever\n",
    "\n",
    "### 4.1 Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Initialize embeddings (uses OpenAI's text-embedding-3-small by default in 2026)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# ALTERNATIVE: Free local embeddings using sentence-transformers\n",
    "# from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "# embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = embeddings.embed_query(\"What is RAG?\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Setting Up the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODERN: Use langchain_chroma (dedicated package)\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Create vector store from documents\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"  # Persists to disk\n",
    ")\n",
    "\n",
    "print(\"Vector store created and persisted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Creating a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # or \"mmr\" for diversity\n",
    "    search_kwargs={\"k\": 3}     # Return top 3 results\n",
    ")\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is LangChain used for?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n{i}. [{doc.metadata.get('source', 'unknown')}]\")\n",
    "    print(f\"   {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Building a RAG Agent (Modern Approach)\n",
    "\n",
    "### Why Use Agents Instead of Chains?\n",
    "\n",
    "The original tutorial used `create_retrieval_chain`. While still valid, the modern approach uses **agents** because:\n",
    "\n",
    "1. **Flexibility**: Agent decides WHEN to retrieve (not every query needs retrieval)\n",
    "2. **Extensibility**: Easy to add more tools (web search, calculator, etc.)\n",
    "3. **Production-ready**: Built on LangGraph with persistence, streaming, etc.\n",
    "4. **Middleware**: v1.1+ supports pre/post model hooks for guardrails\n",
    "\n",
    "### 5.1 Creating a Retriever Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain v1.2.4: create_retriever_tool is now in langchain_core.tools\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "# Wrap the retriever as a tool the agent can use\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"knowledge_base_search\",\n",
    "    description=\"\"\"Search the knowledge base for information about LangChain, \n",
    "    RAG, vector databases, and embeddings. Use this tool when you need to \n",
    "    find specific information from the documents.\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"Tool created: {retriever_tool.name}\")\n",
    "print(f\"Description: {retriever_tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Creating the RAG Agent\n",
    "\n",
    "Now we use the modern `create_agent` API from `langchain.agents` (LangChain v1.2.4, January 2026)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain v1.2.4 (2026): Using create_agent from langgraph.prebuilt\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define the system prompt for the RAG agent\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant with access to a knowledge base.\n",
    "\n",
    "When answering questions:\n",
    "1. ALWAYS use the knowledge_base_search tool to find relevant information\n",
    "2. Base your answers on the retrieved documents\n",
    "3. If the information isn't in the knowledge base, say so honestly\n",
    "4. Cite the source when providing information\n",
    "\n",
    "Be concise but thorough in your responses.\n",
    "\"\"\"\n",
    "\n",
    "# Create the model instance\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Create the agent\n",
    "# NOTE: create_agent comes from langgraph.prebuilt\n",
    "# The system_prompt parameter adds the system prompt\n",
    "rag_agent = create_agent(\n",
    "    model=model,                         # LangChain model instance\n",
    "    tools=[retriever_tool],              # List of tools the agent can use\n",
    "    system_prompt=RAG_SYSTEM_PROMPT     # System prompt as string\n",
    ")\n",
    "\n",
    "print(\"RAG Agent created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Using the RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the agent with a question\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "question = \"What is RAG and how does it work?\"\n",
    "\n",
    "response = rag_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=question)]\n",
    "})\n",
    "\n",
    "# Extract the final answer\n",
    "# The response contains the full message history\n",
    "final_message = response[\"messages\"][-1]\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nAnswer:\\n{final_message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example query\n",
    "question2 = \"What are the popular vector databases I can use?\"\n",
    "\n",
    "response2 = rag_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=question2)]\n",
    "})\n",
    "\n",
    "print(f\"Question: {question2}\")\n",
    "print(f\"\\nAnswer:\\n{response2['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Conversational RAG with Memory\n",
    "\n",
    "For a true chatbot experience, we need conversation memory.\n",
    "\n",
    "### 6.1 Adding Memory with Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT: InMemorySaver from langgraph.checkpoint.memory\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Create a memory saver for persistence\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# Create agent with memory (LangChain v1.2.4)\n",
    "# NOTE: create_agent comes from langgraph.prebuilt\n",
    "conversational_agent = create_agent(\n",
    "    model=model,                          # Reuse the model from above\n",
    "    tools=[retriever_tool],\n",
    "    system_prompt=RAG_SYSTEM_PROMPT,\n",
    "    checkpointer=checkpointer,            # Enable conversation persistence\n",
    ")\n",
    "\n",
    "print(\"Conversational agent with memory created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Multi-Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a session/thread ID for conversation tracking\n",
    "config = {\"configurable\": {\"thread_id\": \"user-session-123\"}}\n",
    "\n",
    "# First message\n",
    "print(\"=\" * 50)\n",
    "print(\"Turn 1\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response1 = conversational_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is LangChain?\")]},\n",
    "    config=config\n",
    ")\n",
    "print(f\"User: What is LangChain?\")\n",
    "print(f\"Assistant: {response1['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up question (agent remembers previous context)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Turn 2 (Follow-up)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response2 = conversational_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"How does it relate to LangGraph?\")]},\n",
    "    config=config  # Same thread_id maintains context\n",
    ")\n",
    "print(f\"User: How does it relate to LangGraph?\")\n",
    "print(f\"Assistant: {response2['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third turn\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Turn 3 (Another follow-up)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "response3 = conversational_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What about vector databases?\")]},\n",
    "    config=config\n",
    ")\n",
    "print(f\"User: What about vector databases?\")\n",
    "print(f\"Assistant: {response3['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Multi-User Support\n",
    "\n",
    "Different users get different conversation contexts by using unique thread IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User A conversation\n",
    "user_a_config = {\"configurable\": {\"thread_id\": \"user-alice-001\"}}\n",
    "\n",
    "response_a = conversational_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about embeddings\")]},\n",
    "    config=user_a_config\n",
    ")\n",
    "print(f\"User Alice: Tell me about embeddings\")\n",
    "print(f\"Assistant: {response_a['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User B has a completely separate conversation\n",
    "user_b_config = {\"configurable\": {\"thread_id\": \"user-bob-002\"}}\n",
    "\n",
    "response_b = conversational_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is RAG?\")]},\n",
    "    config=user_b_config\n",
    ")\n",
    "print(f\"\\nUser Bob: What is RAG?\")\n",
    "print(f\"Assistant: {response_b['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Query Condensing for Follow-up Questions\n",
    "\n",
    "> **The Problem**: When a user asks *\"How does it work?\"* after discussing LangChain, the retriever only sees the literal text \"How does it work?\" â€” without any context about what \"it\" refers to. This leads to poor or irrelevant retrieval results.\n",
    "\n",
    "**Solution**: Use a **query condenser** (also called a *history-aware retriever*) that automatically rewrites follow-up questions into standalone queries by incorporating conversation history.\n",
    "\n",
    "| Turn | User Question | Condensed Query |\n",
    "|------|---------------|-----------------|\n",
    "| 1 | \"What is LangChain?\" | \"What is LangChain?\" |\n",
    "| 2 | \"How does it work?\" | \"How does LangChain work?\" |\n",
    "| 3 | \"What about with vector DBs?\" | \"How does LangChain work with vector databases?\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the Condensing Prompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "CONDENSE_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Given a chat history and the latest user question, \n",
    "rewrite the question to be a standalone query that captures the full context.\n",
    "\n",
    "Rules:\n",
    "- If the question references previous context (e.g., \"it\", \"that\", \"this\"), expand it\n",
    "- If the question is already standalone, return it unchanged\n",
    "- Only output the rewritten question, nothing else\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the query condensing chain\n",
    "query_condenser = CONDENSE_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "print(\"âœ… Query condenser chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create History-Aware Retriever\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "# This wraps the retriever to first condense the query using chat history\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    prompt=CONDENSE_PROMPT\n",
    ")\n",
    "\n",
    "print(\"âœ… History-aware retriever created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build Agent with History-Aware Retrieval\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Create a tool that uses the history-aware retriever\n",
    "condensing_retriever_tool = create_retriever_tool(\n",
    "    retriever=history_aware_retriever,\n",
    "    name=\"search_documents_with_context\",\n",
    "    description=\"Search documents using a context-aware query. Use this for follow-up questions.\"\n",
    ")\n",
    "\n",
    "# Build the agent with the new tool\n",
    "agent_with_condensing = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[condensing_retriever_tool],\n",
    "    prompt=\"You are a helpful assistant. Use the search tool to answer questions.\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Agent with query condensing ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Test Multi-Turn Conversation\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Simulate a 3-turn conversation demonstrating query condensing\n",
    "chat_history = []\n",
    "test_questions = [\n",
    "    \"What is LangChain?\",\n",
    "    \"How does it work?\",           # \"it\" â†’ LangChain\n",
    "    \"What about vector databases?\" # continues context\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ”„ MULTI-TURN CONVERSATION TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nðŸ‘¤ Turn {i}: {question}\")\n",
    "    \n",
    "    # Invoke agent with chat history\n",
    "    response = agent_with_condensing.invoke({\n",
    "        \"messages\": chat_history + [HumanMessage(content=question)]\n",
    "    })\n",
    "    \n",
    "    # Extract and display response\n",
    "    answer = response[\"messages\"][-1].content\n",
    "    print(f\"ðŸ¤– Assistant: {answer[:200]}...\" if len(answer) > 200 else f\"ðŸ¤– Assistant: {answer}\")\n",
    "    \n",
    "    # Update chat history\n",
    "    chat_history.extend([\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=answer)\n",
    "    ])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Query condensing working - follow-up questions understood!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Production Considerations\n",
    "\n",
    "### 7.1 Streaming Responses\n",
    "\n",
    "For better UX, stream responses token-by-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming example (correct v1.2.4 pattern)\n",
    "print(\"Streaming response:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Use stream_mode=\"values\" for cleaner message access\n",
    "for step in conversational_agent.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Give me a summary of what I've learned\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"user-session-123\"}},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    # Access the last message in the step\n",
    "    last_message = step[\"messages\"][-1]\n",
    "    if hasattr(last_message, \"content\") and last_message.content:\n",
    "        # For final AI response, print it\n",
    "        if isinstance(last_message, AIMessage):\n",
    "            print(last_message.content)\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Middleware (NEW in LangChain v1.1+)\n",
    "\n",
    "Middleware allows you to add pre/post processing hooks to your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Pre-processing with custom node wrapper\n",
    "# LangGraph uses custom nodes for middleware-like behavior\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "def with_logging(agent):\n",
    "    \"\"\"Wrapper that adds logging around agent invocations.\"\"\"\n",
    "    original_invoke = agent.invoke\n",
    "    \n",
    "    def logged_invoke(inputs, config=None, **kwargs):\n",
    "        print(f\"[LOG] Processing {len(inputs.get('messages', []))} messages...\")\n",
    "        result = original_invoke(inputs, config, **kwargs)\n",
    "        print(f\"[LOG] Response received with {len(result.get('messages', []))} messages.\")\n",
    "        return result\n",
    "    \n",
    "    agent.invoke = logged_invoke\n",
    "    return agent\n",
    "\n",
    "# Create agent with logging wrapper\n",
    "agent_with_logging = with_logging(create_agent(\n",
    "    model=model,\n",
    "    tools=[retriever_tool],\n",
    "    system_prompt=RAG_SYSTEM_PROMPT,\n",
    "))\n",
    "\n",
    "# Test it\n",
    "print(\"Testing agent with logging:\")\n",
    "result = agent_with_logging.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What is LangChain?\")]\n",
    "})\n",
    "print(f\"Answer: {result['messages'][-1].content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Adding More Tools\n",
    "\n",
    "One advantage of the agent approach is easy tool extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# Custom tool example: Get current date\n",
    "@tool\n",
    "def get_current_date() -> str:\n",
    "    \"\"\"Get the current date. Use when asked about today's date.\"\"\"\n",
    "    from datetime import date\n",
    "    return date.today().strftime(\"%B %d, %Y\")\n",
    "\n",
    "# Create agent with multiple tools\n",
    "multi_tool_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[retriever_tool, get_current_date],\n",
    "    system_prompt=\"\"\"You are a helpful assistant with access to:\n",
    "    1. A knowledge base about LangChain and RAG (knowledge_base_search)\n",
    "    2. A tool to get the current date (get_current_date)\n",
    "    \n",
    "    Use the appropriate tool based on the user's question.\"\"\"\n",
    ")\n",
    "\n",
    "# Test with a date question\n",
    "response = multi_tool_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What's today's date?\")]\n",
    "})\n",
    "print(f\"Response: {response['messages'][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_query(agent, question: str, config: dict = None) -> str:\n",
    "    \"\"\"Safely query the agent with error handling.\"\"\"\n",
    "    try:\n",
    "        response = agent.invoke(\n",
    "            {\"messages\": [HumanMessage(content=question)]},\n",
    "            config=config or {}\n",
    "        )\n",
    "        return response[\"messages\"][-1].content\n",
    "    except Exception as e:\n",
    "        return f\"Error processing query: {str(e)}\"\n",
    "\n",
    "# Usage\n",
    "result = safe_query(rag_agent, \"What frameworks are mentioned in the docs?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this tutorial (updated for **LangChain v1.2.4, January 2026**), we covered:\n",
    "\n",
    "1. **LangChain Basics**: LLMs, prompts, parsers, and LCEL chains\n",
    "2. **Document Processing**: Loading and splitting documents (including vision-based PDF extraction)\n",
    "3. **Vector Stores**: Creating embeddings and setting up Chroma\n",
    "4. **RAG Agents**: Using the modern `create_react_agent` + `create_retriever_tool` pattern\n",
    "5. **Conversation Memory**: Multi-turn conversations with `InMemorySaver`\n",
    "6. **Query Condensing**: History-aware retriever for better follow-up question handling\n",
    "7. **Production Features**: Streaming, custom wrappers, multi-tool agents\n",
    "\n",
    "### Key Takeaways (v1.2.4 Updates)\n",
    "\n",
    "| Old Pattern (Pre-v1) | Modern Pattern (v1.2.4) |\n",
    "|------------------------|----------------------------|\n",
    "| `create_retrieval_chain` | `create_react_agent` from `langgraph.prebuilt` + retriever tool |\n",
    "| Manual history management | `InMemorySaver` from `langgraph.checkpoint.memory` |\n",
    "| `langchain_community.vectorstores` | `langchain_chroma` |\n",
    "| No state_modifier | `state_modifier=system_prompt` for system instructions |\n",
    "| Complex memory setup | Simple `checkpointer=InMemorySaver()` |\n",
    "| No query condensing | `create_history_aware_retriever` for follow-ups |\n",
    "\n",
    "### Key Imports Summary (v1.2.4)\n",
    "\n",
    "```python\n",
    "# Agent creation\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import create_retriever_tool  # v1.2.4: moved to langchain_core\n",
    "from langchain_core.tools import tool  # For custom tools\n",
    "\n",
    "# Query Condensing (for follow-up questions)\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Models - Choose ONE provider:\n",
    "# OpenAI:\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # text\n",
    "vision_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)  # vision\n",
    "\n",
    "# OR Ollama (local):\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "llm = ChatOllama(model=\"llama3.2\")  # text: llama3.2, mistral, deepseek-r1\n",
    "llm_vision = ChatOllama(model=\"llava\")  # vision: llava, bakllava\n",
    "\n",
    "# Memory/Checkpointing\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Vector stores & Embeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings  # or OllamaEmbeddings\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "- Add more document loaders (web pages, databases)\n",
    "- Implement user authentication for thread management\n",
    "- Add observability with LangSmith\n",
    "- Deploy as an API with FastAPI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
